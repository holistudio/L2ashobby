{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3bd78c",
   "metadata": {},
   "source": [
    "# Spiraling out of Neural Nets\n",
    "\n",
    "**Dan Lu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60935b",
   "metadata": {},
   "source": [
    "## Spiral Point Classification\n",
    "\n",
    "Here's the scary looking parametric function:\n",
    "\n",
    "$$\n",
    "X_k(t) = t \\left( \\begin{array}{c} {\\sin \\left[ {{2 \\pi} \\over {K}} (2t + k - 1) \\right]} \\\\ {\\cos \\left[ {{2 \\pi} \\over {K}} (2t + k - 1) \\right]} \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc9fa0",
   "metadata": {},
   "source": [
    "#### do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.functional as F\n",
    "\n",
    "import fastbook\n",
    "from fastbook import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "K = 3     # num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spirals\n",
    "\n",
    "t = torch.linspace(0, 1, N)\n",
    "a = 0.8 * t + 0.2  # amplitude 0.2 → 1.0\n",
    "X = list()\n",
    "y = list()\n",
    "for k in range(K):\n",
    "    θ = (2 * t + k) * 2 * torch.pi / K + 0.2 * torch.randn(N)\n",
    "    X.append(torch.stack((a * θ.sin(), a * θ.cos()), dim=1))\n",
    "    y.append(torch.zeros(N, dtype=torch.long).fill_(k))\n",
    "X = torch.cat(X)\n",
    "y = torch.cat(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bb5c8",
   "metadata": {},
   "source": [
    "#### show stuff\n",
    "\n",
    "Here's the pretty looking plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374856b",
   "metadata": {},
   "source": [
    "### The Task\n",
    "\n",
    "Give a neural net:\n",
    "- the $X = (x, y)$ coordinates of each point \n",
    "- predict which of the $K=3$ spirals it belongs to.\n",
    "\n",
    "\n",
    "#### This is non-trivial because spiral points are not linearly separable!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeefa53b",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "- \n",
    "-\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a31b7a",
   "metadata": {},
   "source": [
    "## Sandwich Analogy\n",
    "\n",
    "> #### $A \\space neural \\space net \\space has \\space to \\space be \\space like \\space a \\space sandwich \\space of \\space linear \\space and \\space nonlinear \\space layers...$\n",
    "\n",
    "- `Linear` layers are like bread.\n",
    "- `ReLU` nonlinear layers are like stuff you put in between bread.\n",
    "- without nonlinear layers, you would just have a stack of bread...AND THAT'S NOT A SANDWICH!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c82ed",
   "metadata": {},
   "source": [
    "<img src=\"lin-nonlin.png\">\n",
    "\n",
    "source: [Scanwich](https://twistedsifter.com/2013/01/scanwiches-cross-sections-of-sandwiches-by-jon-chonko/) (a great website!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82137466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # BREAD\n",
    "            nn.Linear(hidden_size, hidden_size), # BREAD            <= NOT A SANDWICH!\n",
    "            nn.Linear(hidden_size,output_size)   # BREAD\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        score = self.net(xb)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # BREAD\n",
    "            nn.ReLU(),                           # STUFF\n",
    "            nn.Linear(hidden_size, hidden_size), # BREAD           <= THAT'S A SANDWICH!\n",
    "            nn.ReLU(),                           # STUFF\n",
    "            nn.Linear(hidden_size,output_size)   # BREAD\n",
    "        )\n",
    "    def forward(self, xb):\n",
    "        score = self.net(xb)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d3382",
   "metadata": {},
   "source": [
    "### do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y, split=0.2):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x_item = self.features[index]\n",
    "        y_item = self.labels[index]\n",
    "        return x_item, y_item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_ds = ToyDataset(X=X, y=y)\n",
    "# len(whole_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccde747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(ds, train_percent=0.8):\n",
    "    \"\"\"\n",
    "    ds - ToyDataset\n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(len(ds) * train_percent)\n",
    "    test_size = len(ds) - train_size\n",
    "\n",
    "    train_subset, test_subset = random_split(ds, [train_size, test_size])\n",
    "\n",
    "    X_train = ds.features[train_subset.indices]\n",
    "    y_train = ds.labels[train_subset.indices]\n",
    "\n",
    "    X_test = ds.features[test_subset.indices]\n",
    "    y_test = ds.labels[test_subset.indices]\n",
    "\n",
    "    train_ds = ToyDataset(X_train,y_train)\n",
    "    test_ds = ToyDataset(X_test,y_test)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357df3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = split_dataset(whole_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452736ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optim):\n",
    "    # set model to train\n",
    "    model.train()\n",
    "\n",
    "    for idx, (xb, yb) in enumerate(train_loader):\n",
    "        # forward pass\n",
    "        score = model(xb)\n",
    "        \n",
    "        # cross-entropy loss \n",
    "        loss = F.cross_entropy(score, yb)\n",
    "\n",
    "        # accuracy\n",
    "        preds = torch.argmax(score, dim=-1)\n",
    "        acc = ((preds - yb) == 0).float().mean().item()\n",
    "\n",
    "        # clear old gradient from previous backprop\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # compute new gradient and backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameter step\n",
    "        optim.step()\n",
    "\n",
    "    # set model back to eval in case \n",
    "    # other code wants to do that by default\n",
    "    model.eval()\n",
    "\n",
    "    return model, loss.item(), acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6774989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader):\n",
    "    \n",
    "    # no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        for idx, (xb, yb) in enumerate(test_loader):\n",
    "            # model output\n",
    "            score = model(xb)\n",
    "\n",
    "            # loss\n",
    "            loss = F.cross_entropy(score, yb)\n",
    "\n",
    "            # accuracy\n",
    "            preds = torch.argmax(score, dim=-1)\n",
    "            acc = ((preds - yb) == 0).float().mean().item()\n",
    "    \n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, train_loader, test_loader, optim, epochs=100, verbose=False):\n",
    "    for e in range(epochs):\n",
    "        model, train_loss, train_acc = train_epoch(model, train_loader, optim)\n",
    "        test_loss, test_acc = test_epoch(model, test_loader)\n",
    "\n",
    "        if verbose:\n",
    "            if (e % 10 == 0) or (e == epochs-1):\n",
    "                print(f\"[Epoch {e}] Train/Test Loss: {train_loss:.2f} / {test_loss:.2f} | Train/Test Acc: {train_acc*100:.1f}% / {test_acc*100:.1f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "linear_model = DeepLinear(input_size = 2, hidden_size = 10, output_size = 3)\n",
    "optim = torch.optim.AdamW(linear_model.parameters(), lr=1e-3)\n",
    "\n",
    "linear_model = train_test(linear_model, train_loader, test_loader, optim, verbose=True, epochs=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model = MLP(input_size = 2, hidden_size = 10, output_size = 3)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "model = train_test(model, train_loader, test_loader, optim, verbose=True, epochs=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_preds = torch.argmax(linear_model(X), dim=-1)\n",
    "preds = torch.argmax(model(X), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c803c67",
   "metadata": {},
   "source": [
    "### show stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all points in side-by-side subplots: model prediction vs ground truth \n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "ax[0].scatter(X[:,0], X[:,1], c=linear_preds)\n",
    "ax[1].scatter(X[:,0], X[:,1], c=preds)\n",
    "\n",
    "ax[0].set_title('Bread Only Deep Network')\n",
    "ax[1].set_title('Sandwich Deep Network')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
    "    a.set_yticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf07ee",
   "metadata": {},
   "source": [
    "#### Without non-linear layers, you will not have a well performing neural network...no matter how \"deep\" it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde4240",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "- \n",
    "-\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50525485",
   "metadata": {},
   "source": [
    "## Bread Dough Analogy\n",
    "\n",
    "> #### $A \\space neural \\space net \\space transforms \\space data \\space like \\space rolling \\space bread \\space dough...$\n",
    "\n",
    "- `Linear` layers **rotate** the data\n",
    "- `ReLU()` non-linear layers **squash** the data\n",
    "- the neural net ulimately learns how much **rotating** + **squashing** + **rotating** + **squashing** to do\n",
    "- UNTIL the data IS linearly separable!\n",
    "\n",
    "<img src=\"rotate-squash.png\">\n",
    "\n",
    "source: [How to knead bread dough](https://www.youtube.com/watch?v=BJiucv88flM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee851d",
   "metadata": {},
   "source": [
    "### How can we visualize this?\n",
    "\n",
    "- the deep neural net's hidden layers are dealing in 10 dimensional space\n",
    "- we can only plot 2D or 3D..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=10, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # BREAD\n",
    "            nn.ReLU(),                           # STUFF\n",
    "            nn.Linear(hidden_size, hidden_size), # BREAD           <= THAT'S A SANDWICH!\n",
    "            nn.ReLU(),                           # STUFF\n",
    "            nn.Linear(hidden_size,output_size)   # BREAD\n",
    "        )\n",
    "    def forward(self, xb):\n",
    "        score = self.net(xb)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf890a",
   "metadata": {},
   "source": [
    "### Solution: Add 2 dimensional bread...\n",
    "\n",
    "Add linear layers that project from 10-D back down to 2-D space (i.e., visualize the 2D embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351daf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPemb(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__() \n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), # BREAD\n",
    "            nn.ReLU(),                          # STUFF\n",
    "            nn.Linear(hidden_size,2),           # THIN BREAD SLICE => GET 2-D output\n",
    "            nn.Linear(2, hidden_size),          # THIN BREAD SLICE                         <= PRETTY MUCH THE SAME SANDWICH\n",
    "            nn.ReLU(),                          # STUFF\n",
    "            nn.Linear(hidden_size,2),           # THIN BREAD SLICE => GET 2-D output\n",
    "            nn.Linear(2,output_size)            # THIN BREAD SLICE\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        score = self.net(xb)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1a9fd",
   "metadata": {},
   "source": [
    "### do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model = MLPemb(input_size = 2, hidden_size = 10, output_size = 3)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "model = train_test(model, train_loader, test_loader, optim, verbose=True, epochs=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef881603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary for storing outputs of any layer\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_outputs(name):\n",
    "    \n",
    "    def hook(model, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    \n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the second to last layer output in 2D\n",
    "\n",
    "# identify the layer we want outputs from\n",
    "layer1 = model.net[2] # layer after first ReLU\n",
    "layer2 = model.net[-2] # second to last layer\n",
    "\n",
    "# register forward hook\n",
    "layer1.register_forward_hook(get_layer_outputs('first_emb'))\n",
    "layer2.register_forward_hook(get_layer_outputs('last_emb'))\n",
    "\n",
    "# forward pass through model\n",
    "# store outputs into layer_outputs dictionary\n",
    "with torch.no_grad():\n",
    "    scores = model(X)\n",
    "\n",
    "h1 = layer_outputs['first_emb']\n",
    "h2 = layer_outputs['last_emb']\n",
    "\n",
    "preds = torch.argmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501b89a",
   "metadata": {},
   "source": [
    "### show stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec50fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all points in side-by-side subplots: model prediction vs ground truth \n",
    "fig, ax = plt.subplots(1,3, figsize=(15,4))\n",
    "\n",
    "ax[0].scatter(X[:,0], X[:,1], c=y)\n",
    "ax[1].scatter(h1[:,0], h1[:,1], c=y)\n",
    "ax[2].scatter(h2[:,0], h2[:,1], c=y)\n",
    "\n",
    "ax[0].set_title('Original Data Points')\n",
    "ax[1].set_title(\"Lin+ReLU Transformed Points\")\n",
    "ax[2].set_title(\"Lin+ReLU+LinReLU Transformed Points\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticks([-1, 0, 1])\n",
    "    a.set_yticks([-1, 0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374dfa5",
   "metadata": {},
   "source": [
    "#### A neural net learns to transform (*rotation + squashing*) the *not-linearly separable* data into *linearly separable* groups.\n",
    "\n",
    "In theory, a neural net can approximate *any given function by* finding a combination of simpler linear and non-linear function parameters to mimic that *given function*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e212bd0",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "- \n",
    "-\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875fbe5",
   "metadata": {},
   "source": [
    "## Creative Coding with Neural Nets\n",
    "\n",
    "> #### $Let's \\space make \\space a \\space rainbow \\space sandwich...$\n",
    "\n",
    "- the neural net doesn't assign physical meaning to the multidimensional data in the hidden layers\n",
    "- the neural net is not trying to make art...\n",
    "- ...but I am :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['x', 'y', 'R', 'G', 'B', 's']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324306f",
   "metadata": {},
   "source": [
    "### do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPdeep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__() # IMPORTANT\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(), # ===============================> 6-D output => map to ['x', 'y', 'R', 'G', 'B', 's']  #1\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(), # ===============================> 6-D output => map to ['x', 'y', 'R', 'G', 'B', 's']  #3\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(), # ===============================> 6-D output => map to ['x', 'y', 'R', 'G', 'B', 's']  #5\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(), # ===============================> 6-D output => map to ['x', 'y', 'R', 'G', 'B', 's']  #7\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(), # ===============================> 6-D output => map to ['x', 'y', 'R', 'G', 'B', 's']  #9\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        score = self.net(xb)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model = MLPdeep(input_size = 2, hidden_size = 6, output_size = 3)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "model = train_test(model, train_loader, test_loader, optim, verbose=True, epochs=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082bac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the second to last layer output in 2D\n",
    "layer_outputs = {}\n",
    "\n",
    "relus = [model.net[i] for i in range(10) if i % 2 == 0]\n",
    "\n",
    "\n",
    "# register forward hook\n",
    "for i, relu in enumerate(relus):\n",
    "    relu.register_forward_hook(get_layer_outputs(f'relu{i+1}'))\n",
    "\n",
    "\n",
    "# forward pass through model\n",
    "# store outputs into layer_outputs dictionary\n",
    "with torch.no_grad():\n",
    "    scores = model(X)\n",
    "    preds = torch.argmax(scores, dim=-1)\n",
    "\n",
    "layer_outputs['relu1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38df3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all points in side-by-side subplots: model prediction vs ground truth \n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "ax[0].scatter(X[:,0], X[:,1], c=y)\n",
    "ax[1].scatter(X[:,0], X[:,1], c=preds)\n",
    "\n",
    "ax[0].set_title('Original Spiral Classes')\n",
    "ax[1].set_title('Network Predicted Spiral Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ae8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(len(features))[:len(features)]\n",
    "feature_map = [features[i] for i in list(idx)]\n",
    "print(idx)\n",
    "print(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572059ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_to_dot_features(h_out, norm=None, idx=None):\n",
    "    \n",
    "    def minmax(x, dim=None, eps=1e-8):\n",
    "        if dim is None:\n",
    "            return (x - x.min()) / (x.max() - x.min()) + eps\n",
    "        x_min = x.min(dim=dim, keepdim=True).values\n",
    "        x_max = x.max(dim=dim, keepdim=True).values\n",
    "        return (x - x_min) / (x_max - x_min) + eps\n",
    "    \n",
    "    # decide on normalization scheme\n",
    "    if norm == None:\n",
    "        h_out = h_out\n",
    "    elif norm == 'softmax':\n",
    "        h_out = torch.softmax(h_out, dim=-1)\n",
    "    elif norm == 'sigmoid':\n",
    "        h_out = torch.sigmoid(h_out)\n",
    "    elif norm == 'minmax':\n",
    "        h_out = minmax(h_out, dim=-1)\n",
    "    else:\n",
    "        return ValueError(\"Invalid norm scheme, choose from ['softmax', 'sigmoid', 'minmax']\")\n",
    "    \n",
    "    if idx == None:\n",
    "        idx = torch.randperm(len(features))[:len(features)]\n",
    "    \n",
    "    feature_map = [features[i] for i in list(idx)]\n",
    "    # print(idx, feature_map)\n",
    "\n",
    "    # Split into separate arrays\n",
    "    x = h_out[:, idx[0]]\n",
    "    y = h_out[:, idx[1]]\n",
    "\n",
    "    colors_r = h_out[:, idx[2]]  \n",
    "    colors_g = h_out[:, idx[3]]  \n",
    "    colors_b = h_out[:, idx[4]]  \n",
    "    colors = torch.stack((colors_r,colors_g,colors_b), dim=1)\n",
    "    \n",
    "    sizes = (h_out[:, idx[5]])\n",
    "    # normalize first (optional but helps keep things consistent)\n",
    "    sizes = (sizes - sizes.min()) / (sizes.max() - sizes.min())\n",
    "\n",
    "    # apply exponential scaling\n",
    "    sizes = torch.exp(3 * sizes) * 100  # tweak the multiplier (3) and base size (20)\n",
    "\n",
    "    if norm == None:\n",
    "        colors = minmax(colors, dim=-1)\n",
    "        sizes = minmax(sizes, dim=-1)\n",
    "\n",
    "    return (x, y, colors, sizes), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_features, idx = hidden_to_dot_features(h_out=layer_outputs['relu1'])\n",
    "x, y, colors_in, sizes_in = dot_features\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    s=sizes_in,\n",
    "    c=colors_in\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_features, idx = hidden_to_dot_features(h_out=layer_outputs['relu2'], idx=idx)\n",
    "x, y, colors_in, sizes_in = dot_features\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    s=sizes_in,\n",
    "    c=colors_in\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sandwich(idx=None, fix_scale=False):\n",
    "\n",
    "\n",
    "    keys = layer_outputs.keys()\n",
    "    n_relus = len(keys)\n",
    "\n",
    "    fig, ax = plt.subplots(n_relus,1, figsize=(5.5, 5.5*n_relus))\n",
    "    # Turn on dark mode manually\n",
    "    fig.patch.set_facecolor(\"black\")      # entire figure background\n",
    "\n",
    "    for a in ax.ravel():\n",
    "        a.set_facecolor(\"black\")\n",
    "        a.tick_params(colors=\"white\")\n",
    "        a.xaxis.label.set_color(\"white\")\n",
    "        a.yaxis.label.set_color(\"white\")\n",
    "        a.title.set_color(\"white\")\n",
    "        for spine in a.spines.values():\n",
    "            spine.set_color(\"white\")\n",
    "\n",
    "    if fix_scale:\n",
    "        # Collect limits first\n",
    "        xmins, xmaxs, ymins, ymaxs = [], [], [], []\n",
    "\n",
    "        for i, k in enumerate(keys):\n",
    "            dot_features, idx = hidden_to_dot_features(h_out=layer_outputs[k], idx=idx)\n",
    "            x, y, colors, sizes = dot_features\n",
    "            xmins.append(x.min())\n",
    "            xmaxs.append(x.max())\n",
    "            ymins.append(y.min())\n",
    "            ymaxs.append(y.max())\n",
    "\n",
    "        # Compute global bounds\n",
    "        xlim = (min(xmins), max(xmaxs))\n",
    "        ylim = (min(ymins), max(ymaxs))\n",
    "\n",
    "    # Now plot with fixed limits\n",
    "    for i, k in enumerate(keys):\n",
    "        dot_features, idx = hidden_to_dot_features(h_out=layer_outputs[k], idx=idx)\n",
    "        x, y, colors, sizes = dot_features\n",
    "\n",
    "        ax[i].scatter(x, y, s=sizes, c=colors)\n",
    "        if fix_scale:\n",
    "            ax[i].set_xlim(xlim)\n",
    "            ax[i].set_ylim(ylim)\n",
    "            ax[i].set_aspect('equal', adjustable='box')\n",
    "        ax[i].set_title(k)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2cb32",
   "metadata": {},
   "source": [
    "### show stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([5, 4, 1, 2, 3, 0]) ['s', 'B', 'y', 'R', 'G', 'x']\n",
    "\n",
    "idx = [5, 4, 1, 2, 3, 0]\n",
    "plot_sandwich(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477abd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([5, 4, 1, 2, 3, 0]) ['s', 'B', 'y', 'R', 'G', 'x']\n",
    "plot_sandwich(idx, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [5, 0, 1, 4, 3, 2]\n",
    "feature_map = [features[i] for i in list(idx)]\n",
    "print(idx)\n",
    "print(feature_map)\n",
    "plot_sandwich(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0, 5, 1, 4, 3, 2]\n",
    "feature_map = [features[i] for i in list(idx)]\n",
    "print(idx)\n",
    "print(feature_map)\n",
    "plot_sandwich(idx, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12947adc",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was largely inspired by Alfredo Canziani when I took the NYU Deep Learning course in Fall 2021. Specifically these two recitation topics really opened up my eyes to how deep neural nets work at a fundamental level:\n",
    "\n",
    " - Spiral classification: [Lecture](https://www.youtube.com/watch?v=EyKiYVwrdjE) | [Notebook](https://github.com/Atcold/NYU-DLSP21/blob/master/04-spiral_classification.ipynb) | [Notes](https://atcold.github.io/NYU-DLSP20/en/week02/02-3/)\n",
    " - Linear vs Non-linear transformations: [Lecture](https://www.youtube.com/watch?v=panJ-pkaqBQ) | [Notebook](https://github.com/Atcold/NYU-DLSP20/blob/master/02-space_stretching.ipynb) | [Notes](https://atcold.github.io/NYU-DLSP20/en/week01/01-3/)\n",
    "\n",
    "I hope that this notebook here distills the key concepts and combines the various details of the above discussions into a single \"spiral show.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
